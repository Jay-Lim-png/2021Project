# 4.1 전처리

- what is corpus?

: 자연어처리에 사용하는, 단어들로 이루어진 문장이 코퍼스이다. 코퍼스는 단일 언어 코퍼스와 이중 언어 코퍼스, 다중 언어 코퍼스로 나눌 수 있음. 병렬 코퍼스는 번역데이터처럼 한 문장에 대해 두 개 이상의 언어로 쓰여진 것을 의미.

e.g) 나는 학교에 가는 것을 좋아한다.

## 1. 전처리 과정의 개요

===전처리 순서 ====

1. 코퍼스 수집
2. 정제
3. 문장 단위 분절
4. 분절
5. 병렬 코퍼스 정렬 (생략 가능)
6. 서브워드 분절

## 

## 2. 정제 과정

### [정규표현식]

- or 표현 [ ] : [2345cde]  또는 (2|3|4|5|c|d|e)
- 연속된 숫자 또는 알파벳 표현 [-] : [2-5c-e]
- Not 표현 [^]

    [^2-5c-e] → 2부터 5까지, c부터 2까지를 제외한 한 글자 

- 그룹 만들기 ( ) : 그룹은 수식에서 ( ) 와 비슷한 역할이라고 생각하면 됨.. ( a + b ) * 5 로 반복 표현을 간단하게 할 수 있는 것처럼, 정규표현식에서의 그룹도 비슷한 역할 수행
- ? : 앞의 수식하는 부분이 나타나지 않거나 한 번만 나타날 때

    그룹에서 ?는 다른 역할을 수행하게 됨

- + : 앞의 수식하는 부분이 한 번 이상 나타날 때

    x+

    ![4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled.png](4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled.png)

- * : 앞의 수식하는 부분이 나타나지 않거나 여러 번 나타날 경우 (이 말을 처음 읽었을 땐 이런 생각이 들었다. ~~그럼 이러든 저러든 상관 없다는 소리 아니야?~~ 뒤에 예시에서 보겠지만 \S와 함께 사용하면서 '있을 수도 있고 없을 수도 있는 문자'를 처리하기 위해서 사용한다. )
- 반복되는 문자 표현
    - x{n} : x가 9번 반복
    - x{n, } : x가 9번 이상 반복
    - x{n,m} : x가 n번에서 m번 사이를 반복
- . 사용 : 어떤 글자도 지칭할 수 있는 표현 (like 루미큐브 조커카드)

![4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled%201.png](4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled%201.png)

정규식 안에서 .을 사용하려면  \. 으로 지정해야 함. 아래 관련문서 참조[https://stackoverflow.com/questions/13989640/regular-expression-to-match-a-dot](https://stackoverflow.com/questions/13989640/regular-expression-to-match-a-dot)

- ^와 $ : [ ] 안에 포함되지 않은 ^와 $는 라인의 시작과 끝을 의미함

    ^x$

    ![4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled%202.png](4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled%202.png)

- 지정문자 사용

[지정문자 ](https://www.notion.so/b0566cd7b6074ad0adb10204f06765f8)

### 정규식 패턴 연습 사이트

[RegexOne - Learn Regular Expressions - Lesson 1: An Introduction, and the ABCs](https://regexone.com/)

### 파이썬에서 정규 표현식 사용하기

re 모듈을 이용해 파이썬에서 활용

```python
import re
regex_pattern = '^a...s$'  #문장의 처음과 끝이 a와 s로 끝나고, 그 사이에 3개의 문자를 포함 
text = 'abyss'

## re 모듈 함수를 통해 텍스트 전처리
re.sub(regex_pattern, 'REPLACED', text)
>> 'REPLACED'
```

re 모듈의 내장함수는 아래 문서를 참조

[https://www.programiz.com/python-programming/regex](https://www.programiz.com/python-programming/regex)

### 치환자 사용

그룹( ) 으로 지정된 부분을 역슬래시\ 또는 $ + 숫자 형식으로 변수명처럼 가리킬 수 있음

```python
x = '''abcdefg
12345
ab12
ab1bc2d
12ab
a1b
1a
hijklmnop
'''

regex_pattern = r'([a-z])[0-9]+([a-z])'
to = r'\1\2'

y = '\n'.join([re.sub(regex_pattern, to, x_i) for x_i in x.split('\n')])
```

위 코드는 x를 엔터 단위로 나눈 단어들에서 *문자(a-z)*와 *문자(a-z)* 사이의 숫자[0-9]가 있을 경우 문자만 남기도록 단어를 바꿔주었다. 

바뀐 후의 단어인 r'\1\2'에서 \1은 첫번째 그룹인 문자(a-z)를 지칭, \2는 두번째 그룹 ([a-z])를 지칭한다. 

## 3. 문장 단위 분절 과정

마침표만을 기준으로 문장 단위 분절을 수행하면 영어 약자나 소수점 등 여러가지 문제에 마주칠 수 있음. 따라서 분절을 위한 모듈을 만들거나 자연어처리 툴킷인 NLTK를 이용

[문장 단위 분절 예제]

자연어처리는 인공지능의 한 줄기 입니다. 시퀀스 투 시퀀스의 등장 이후로 딥러닝을 활용한 자연어처리는 새로운 전기를 맞이하게 되었습니다. 문장을 받아 단순히 수치로 나타내던 시절을 넘어, 원하는대로 문장을 만들어낼 수 있게 된 것입니다.

```python
import sys, fileinput, re
from nltk.tokenize import sent_tokenize

if __name__ == "__main__":
    for line in fileinput.FileInput('nlp with pytorch chap4.txt'):
        if line.strip() !="": #line.strip() -> 공백을 지우고 문자열을 반환
            line = re.sub(r'([a-z])\.([A-Z])',r'\1. \2', line.strip())
            
            sentences = sent_tokenize(line.strip())
            
        for s in sentences:
            if s!= "":
                sys.stdout.write(s + "\n")
```

위 코드는 [a-z]와 [a-z] 사이에 . 이 있으면 해당 문자열을 [a-z]. [a-z]로 반환한다. 내가 이해하기로는 문장 말미에 마침표를 찍고 띄어쓰기를 하는 문장이 있고, 하지 않는 문장이 있어서 전부 한 칸 띄워주는 걸로 바꾸는 것 같다. 혹은 sent_tokenize가 마침표+띄어쓰기만 문장 단위로 인식한다거나?

찾아보니 sent_tokenize는 regular expression을 이용하는 Treebank tokenizer 방법을 사용한다. 이 Treebank tokenizer는 텍스트가 이미 문장 단위로 분리되어있다고 가정한다. 

[문장 합치기 예제]

자연어처리는 인공지능의 한 줄기 입니다. 시퀀스 투 시퀀스의 등장 이후로 \n
딥러닝을 활용한 자연어처리는 새로운 전기를 맞이하게 되었습니다. 문장을\n
받아 단순히 수치로 나타내던 시절을 넘어, 원하는대로 문장을 만들어낼 수 \n
있게 된 것입니다.

```python
import sys, fileinput
from nltk.tokenize import sent_tokenize

if __name__ == "__main__":
    buf = []

    for line in fileinput.input(data):
        if line.strip() != "":
            buf += [line.strip()]
            sentences = sent_tokenize(" ".join(buf))

            if len(sentences) > 1:
                buf = sentences[1:]

                sys.stdout.write(sentences[0] + '\n')

    sys.stdout.write(" ".join(buf) + "\n")
```

위 코드는 문장을 하나로 합친 뒤에, sent_tokenize를 이용하여 이번에는 제대로 된 문장 단위로 분리해주게 된다. 

## 4. 단어 단위의 분절

[한국어 분절]

전형적인 문장에서는 프로그램마다 성능이 비슷하지만, 신조어나 고유명사를 처리할 때 신조어마다 처리 정책이 다르다. 따라서 본인이 가진 텍스트의 성질을 잘 파악하여 프로그램을 선택해야 함

- Mecab

    : 가장 속도가 빠름

    윈도우에서는 사용이 불가능함

    설치는 아래 참조

    [설치하기 - KoNLPy 0.4.3 documentation](https://konlpy-ko.readthedocs.io/ko/v0.4.3/install/#ubuntu)

- KoNLPy

    내부 라이브러리들이 각기 다른 언어로 이루어져 호환성 문제가 발생하거나, 일부 라이브러리들이 자바로 구현되어 Mecab에 비해 대용량 코퍼스 처리에 불리함 (속도 이슈)

    그러나 설치가 자유로워 널리 이용

    각 KoNLPy 하위 클래스의 특징은 아래를 참조

    [tag Package - KoNLPy 0.5.2 documentation](https://konlpy.org/ko/latest/api/konlpy.tag/#module-konlpy.tag._hannanum)


#5. 유사성과 모호성
    
   
자연어의 중의성 문제를 어떻게 해결할 것인가? 

'차'는 car, tea, difference, time의 의미를 가지고 있다. 이 서로 다른 뜻을 '연관의미'라고 부른다. 같은 연관의미 안에서 다시 세부의미가 나뉜다. 예를 들어, '차이'와 동일한 연관의미로 사용할 때, 이 차이는 수준을 나타내는 말일 수도 있고, 두 숫자의 뺄셈을 나타낸 것일 수도 있다. 이렇게 연관의미 내부에서 달라지는 의미를 '세부의미'라고 부른다. 

자연어 기반의 인공지능은 동일한 단어를 세부 의미로 변환하는 과정이 필요하다.

## 시소러스를 활용한 단어 의미 파악

단어의 계층구조를 데이터베이스로 구축한 것을 thesaurus(어휘분류사전)이라고 부름

### 워드넷

[http://wordnetweb.princeton.edu/perl/webwn](http://wordnetweb.princeton.edu/perl/webwn)

단어의 상위어와 하위어를 정리하여 단어의 중의성 해소

현재는 워드 임베딩을 사용하여 단어체계(온톨로지)를 어느정도 정립할 수 있기 때문에 현재 많이 사용하지는 않음

[한국어 워드넷]

http://wordnet.kaist.ac.kr/

http://korlex.pusan.ac.kr/

### TF-IDF 특징 추출

TF-IDF는 문서에서 특정 단어가 가진 중요도를 나타내는 수치이다.

$$TF-IDF(w,d)= \frac {TF(w,d)}{IDF(w)}$$

- TF(term frequency) : 단어의 문서 내에 출현한 횟수
- IDF(inverse document frequency) : 단어가 출현한 문서의 숫자 역수

단어의 출현횟수에 $\frac {1}{IDF(w)}$을 곱해줌으로써 'the'와 같은 단어의 중요도가 커지는 것을 막는다. 

역문서빈도인 IDF식은 보통 아래와 같은 식으로 구하게 된다. 

$${\mathrm {idf}}(t,D)=\log {\frac {|D|}{|\{d\in D:t\in d\}|}}$$

D는 전체 문서의 개수, 분모는 개별 문서 d 중 단어 t를 포함하고 있는 문서의 수가 된다.

### TF-IDF 계산 코드(1) - TF계산

```python
import pandas as pd

## 문서 내 단어의 출현빈도 세기
def get_term_frequency(document, word_dict = None):
    if word_dict is None:
        word_dict = {}
    words = document.split()
    '''
    str.split(sep=None, maxsplit=-1)
    https://docs.python.org/ko/3/library/stdtypes.html?highlight=split#str.split
    문자열을 구분자 기준으로 나누어서 maxsplit+1개의 요소를 가진 리스트로 반환함
    '''
    
    for w in words:
        word_dict[w] = 1 + (0 if word_dict.get(w) is None else word_dict[w])
    '''
    words 리스트 요소들을 key로 추가하면서 셈
    특정 단어 최초 등장시 key 값은 1 1 + (0 if word_dict.get(w) is None)
    두번 이상 등장 시 key 값은 1 + word_dict[w](이전까지 등장한 총합)
    '''
        
    return pd.Series(word_dict).sort_values(ascending = False)
		'''
    index:단어, value:등장 횟수 인 Series로 return
    '''
```

미리 만들어놓은 문자열 doc1에 위 함수를 실행하면, 다음과 같이 index는 구분자로 나누어진 문자열이고, value는 해당 문자의 문서 내 빈도가 되는 series가 생긴다. 

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a4b3e8da-469d-4466-9352-258a37678968/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a4b3e8da-469d-4466-9352-258a37678968/Untitled.png)

..... 난 여기서 이해가 안가고 시간을 많이 뺏어먹은 게, 대체 왜 word_dict를 그냥 dictionary 형태로 return을 안하고 굳이 series 형태로 리턴한거지? dictionary도 value값을 기준으로 sort 가능한 것 같은데? ([https://careerkarma.com/blog/python-sort-a-dictionary-by-value/](https://careerkarma.com/blog/python-sort-a-dictionary-by-value/))

그러면 sort한 다음에 가장 큰 값부터 iterable로 만들어서 print하는 건 어떨까? 그게 더 시간이 적게 걸리지 않을까?

→ 아 Series로 value_counts를 하는 게 dictionary로 계산하는 거보다 빨라서 그렇구나..

## series와 dictionary의 차이 (series를 사용하는 이유)

[내가 궁금했던 내용] 

🤔 series는 dictionary처럼 쓸 수도 있고, list처럼도 쓸 수 있고, 데이터프레임의 한 열로도 생각할 수 있는 듯 하다. 그럼 series는 대체 왜 있는건가.. 라는 생각이 드는데. .. 

🤔  list와 dictionary와 구분되는 series의 특징은 series는 동일한 type의 데이터로만 구성되어야 한다는 것이다. dataframe은 series의 집합이다. →  그런데 실제로는 index든 value든 섞어 사용 가능...

[해결]

- pandas 안에 series 를 이용해서 계산하는 방법이 dictionary보다 훨씬 빠르다고 한다.
- series는 계산할 때 vectorize해서 계산하기 때문에 dictionary가 계산 처리하는 방법보다 빠름 (대신 메모리를 더 많이 먹는다고 함)
- Series의 특징은 하나의 data type만 들어갈 수 있다는 것인데, series의 index든 value든 모두 str, int가 동시에 값으로 들어갈 수 있다. Series의 type은 row 하나당 정해지는 것이 아니고 column 전체를 기준으로 정해진다. 만약 str과 int형이 동시에 들어가 있다면 column의 series type은 object가 된다 (여러 값이 섞여있으면 그냥 object로 묶어서 생각해버림)

### TF-IDF 계산 (2) - IDF 계산

```python
## 각 단어의 몇 개의 문서에서 나타났는지 (IDF) 세기
def get_document_frequency(documents):
    dicts = []
    vocab = set([])
    df = {}
    
    for d in documents:
        tf = get_term_frequency(d) # tf = document d의 단어별 등장횟수 series
        dicts += [tf] # list에 각 document의 단어 빈도수 Series를 요소로 추가
        vocab = vocab | set(tf.keys())  ## set | set 은 합집합을 return
	'''
	tf.keys() -> keys는 Series의 index를 return하는 메서드임 (메서드는 class안에서 정의된 함수)
	set(tf.keys())는 집합이 되고, set|set 은 합집합을 return함
	for문을 마친 vocab은 모든 document에 있는 단어의 집합이 됨.
	'''
         
    for v in list(vocab):
        df[v] = 0  # df[v]에 단어 v가 몇개의 문서에서 나타났는지 저장
        for dict_d in dicts: 
            if dict_d.get(v) is not None:  # get 메서드는 dictionary처럼 key의 value를 반환
                df[v] += 1  # i번째 문서인 dict_d에 단어 v가 있다면 IDF에 +1 처리
                
    return pd.Series(df).sort_values(ascending=False)
```

### TF-IDF의 최종 계산

```python
## TF-IDF의 최종 함수

def get_tfidf(docs): ## 각 문서를 list로 받음
    vocab = {}
    tfs = []
    for d in docs:
        vocab = get_term_frequency(d, vocab)   # vocab = 단어 빈도 Series (TF)
        tfs += [get_term_frequency(d)]  # 문서 순서대로 TF Series를 list로 저장
    df = get_document_frequency(docs)  # 단어의 IDF

    from operator import itemgetter
    '''
    f = itemgetter(2), the call f(r) returns r[2].
    g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3]).
    '''
    import numpy as np

    stats = []
    for word, freq in vocab.items():
        tfidfs = []
        for idx in range(len(docs)):
            if tfs[idx].get(word) is not None:  ## i번째 문서 단어열(tfs[idx])에서 특정 단어의 빈도(TF)가 0이 아니면
                tfidfs += [tfs[idx][word] * np.log(len(docs) / df[word])] ## TF * 역문서빈도 log(문서개수/단어가 문서에 나타난 빈도)
            else:
                tfidfs += [0] ## 만약 빈도가 0이면 TF-IDF는 분자가 0이므로 계산 필요성 X

        stats.append((word, freq, *tfidfs, max(tfidfs)))

    return pd.DataFrame(stats, columns=('word',
                                        'frequency',
                                        'doc1',
                                        'doc2',
                                        'doc3',
                                        'max')).sort_values('max', ascending=False)

get_tfidf([doc1, doc2, doc3])
```

아래와 같은 결과값을 얻을 수 있음

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9985acfd-f437-4553-9582-ba9d1de34dec/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9985acfd-f437-4553-9582-ba9d1de34dec/Untitled.png)

## 특징 벡터 만들기

### TF 행렬 만들기 (단어 빈도수 행렬)

- 문서별 단어의 출현 횟수인 TF를 차원별로 구성해 문서 간 관계를 나타내는 특징 벡터로 활용

```python
def get_tf(docs):
    vocab = {} 
    tfs = []
    for d in docs:
        vocab = get_term_frequency(d, vocab)
        tfs +=  [get_term_frequency(d)] ## doc 개수만큼 tf series를 가진 리스트
        
    from operator import itemgetter
    import numpy as np
    
    stats = []
    for word, freq in vocab.items():
        tf_v = []
        for idx in range(len(docs)):
            if tfs[idx].get(word) is not None:
                tf_v += [tfs[idx][word]]
            else:
                tf_v += [0]
        stats.append((word, freq, *tf_v))

return pd.DataFrame(stats, columns = ('word', 
                                          'frequency', 
                                          'doc1', 
                                          'doc2', 
                                          'doc3')).sort_values('frequency', ascending = False)
```

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/82de5a69-5901-4008-ad5f-c776ebce5f75/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/82de5a69-5901-4008-ad5f-c776ebce5f75/Untitled.png)

- doc1, doc2, doc3에 해당하는 TF(w,doc1), TF(w,doc2) ... 의 3개 벡터는 단어의 출현횟수를 정의한 특징 벡터가 된다.
- 그러나 이러한 특징 벡터는 문서가 커지면 커질수록 차원이 너무 커지고, 희소한 행렬(대부분이 0으로 채워짐)이 된다는 문제점이 있다.

### Context Window를 사용한 단어 정보 활용

- TF를 특징벡터로 사용하는 경우 단순히 단어의 빈도수만을 나열했기 때문에 문장의 맥락 등 부가적인 정보가 유실된다. 이러한 문제를 막기 위해 "동시발생"하는 단어들을
